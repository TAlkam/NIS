{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNDVVsWPj2IuyccHBa7UXY7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TAlkam/NIS/blob/main/Main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## ðŸ“˜ Notebook Outline\n",
        "\n",
        "This notebook presents a dual-method analysis of in-hospital mortality in Alzheimerâ€™s disease (AD) patients using the 2017 NIS dataset. It includes data preprocessing, logistic regression, XGBoost modeling, and SHAP explainability.\n",
        "\n",
        "### 1. **Setup and Imports**\n",
        "\n",
        "* Install and import required Python packages.\n",
        "\n",
        "### 2. **Data Loading**\n",
        "\n",
        "* Load `.dta` file using `pyreadstat`.\n",
        "* Preview dataset dimensions and contents.\n",
        "\n",
        "### 3. **Diagnosis Code Extraction**\n",
        "\n",
        "* Extract and flatten `I10_DX1â€“I10_DX40` diagnosis columns.\n",
        "* Exclude Alzheimerâ€™s/dementia and Z-codes (optional).\n",
        "* Merge with ICD-10 descriptions.\n",
        "\n",
        "### 4. **Top Diagnosis Frequency Analysis**\n",
        "\n",
        "* Count and display top 30 most frequent diagnoses.\n",
        "\n",
        "### 5. **Feature Engineering**\n",
        "\n",
        "* Define binary, categorical, and continuous variables.\n",
        "* Create target variable (`died`).\n",
        "* Preprocess features for modeling.\n",
        "\n",
        "### 6. **Modeling: Logistic Regression & XGBoost**\n",
        "\n",
        "* Apply stratified train-validation-test split.\n",
        "* Train logistic regression (with survey weights).\n",
        "* Train XGBoost classifier with early stopping.\n",
        "\n",
        "### 7. **Model Evaluation**\n",
        "\n",
        "* Report AUROC, AUPRC, Brier Score, and LogLoss.\n",
        "* Cross-validation performance metrics.\n",
        "\n",
        "### 8. **Model Explainability (SHAP)**\n",
        "\n",
        "* Generate SHAP summary and bar plots for XGBoost.\n",
        "* Identify key features contributing to mortality prediction.\n",
        "\n",
        "### 9. **Sensitivity Analysis**\n",
        "\n",
        "* Re-train models after excluding `dnr` and `pall` variables.\n",
        "* Recompute SHAP and evaluate performance change.\n",
        "\n",
        "### 10. **Export and Save**\n",
        "\n",
        "* Save final model, SHAP plots, and top features.\n"
      ],
      "metadata": {
        "id": "DNVY_o1-Hll8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install required packages\n",
        "!pip install pyreadstat pandas openpyxl --quiet\n",
        "\n",
        "# ðŸ“š Import libraries\n",
        "import pandas as pd\n",
        "import pyreadstat\n",
        "\n",
        "# ðŸ“‚ Load your .dta file\n",
        "df, meta = pyreadstat.read_dta('/content/NIS_2017_Core data age_over_60_alz_only-DIED.dta')\n",
        "\n",
        "# âœ… Step 1: Extract all I10_DX* diagnosis code columns\n",
        "dx_cols = [col for col in df.columns if col.lower().startswith('i10_dx')]\n",
        "print(f\"Diagnosis columns: {dx_cols[:5]} ... total: {len(dx_cols)}\")\n",
        "\n",
        "# âœ… Step 2: Stack all diagnosis codes into a single Series\n",
        "all_dx = df[dx_cols].values.ravel()  # Flatten all diagnosis codes\n",
        "all_dx = pd.Series(all_dx).dropna().astype(str).str.upper().str.strip()\n",
        "\n",
        "# âŒ DO NOT exclude dementia, Z-codes, or anything else â€” include all ICD-10 codes\n",
        "# âœ… Step 3: Count frequencies of ALL ICD-10 diagnosis codes\n",
        "dx_counts = all_dx.value_counts().reset_index()\n",
        "dx_counts.columns = ['icd10_code', 'count']\n",
        "dx_counts['percent'] = dx_counts['count'] / dx_counts['count'].sum() * 100\n",
        "\n",
        "# ðŸ“‚ Step 4: Load ICDâ€‘10 code descriptions (make sure filename is correct)\n",
        "icd_lookup = pd.read_excel('/content/section111validicd10-jan2025_0.xlsx')\n",
        "\n",
        "# ðŸ§¹ Step 5: Clean and standardize\n",
        "icd_lookup.columns = icd_lookup.columns.str.lower().str.strip()\n",
        "icd_lookup = icd_lookup.rename(columns={\n",
        "    'code': 'icd10_code',\n",
        "    'long description (valid icd-10 fy2025)': 'description'\n",
        "})\n",
        "icd_lookup['icd10_code'] = icd_lookup['icd10_code'].str.upper().str.strip()\n",
        "dx_counts['icd10_code'] = dx_counts['icd10_code'].str.upper().str.strip()\n",
        "\n",
        "# ðŸ”— Step 6: Merge frequencies with ICD-10 descriptions\n",
        "merged = dx_counts.merge(icd_lookup[['icd10_code', 'description']], on='icd10_code', how='left')\n",
        "\n",
        "# ðŸ§½ Step 7: Optionally drop missing descriptions, or keep them for manual review\n",
        "merged = merged.dropna(subset=['description'])  # or comment this out to keep all rows\n",
        "\n",
        "# ðŸ“Š Step 8: Display top 30 most frequent diagnosis codes (now includes Z-codes)\n",
        "merged_top30 = merged[['icd10_code', 'description', 'count', 'percent']].head(30)\n",
        "display(merged_top30)\n"
      ],
      "metadata": {
        "id": "GXl7K2Wf7ja5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-EfIH3bA7tJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ðŸ“¦ Install required packages\n",
        "!pip install pyreadstat pandas openpyxl --quiet\n",
        "\n",
        "# ðŸ“š Import libraries\n",
        "import pandas as pd\n",
        "import pyreadstat\n",
        "\n",
        "# ðŸ“‚ Load your .dta file\n",
        "df, meta = pyreadstat.read_dta('/content/NIS_2017_Core data age_over_60_alz_only-DIED.dta')  # Update path if needed\n",
        "\n",
        "# âœ… Step 1: Extract all I10_DX* columns (diagnosis codes)\n",
        "dx_cols = [col for col in df.columns if col.lower().startswith('i10_dx')]\n",
        "\n",
        "# âœ… Step 2: Stack all diagnosis codes into a single Series\n",
        "all_dx = df[dx_cols].values.ravel()  # Flatten 2D array\n",
        "all_dx = pd.Series(all_dx).dropna().str.upper().str.strip()\n",
        "\n",
        "# âœ… Step 3: Exclude dementia and Alzheimer's codes (F01*, F02*, F03*, G30*, Z-codes)\n",
        "exclude_prefixes = ['F01', 'F02', 'F03', 'G30']\n",
        "all_dx_filtered = all_dx[~all_dx.str.startswith(tuple(exclude_prefixes))]\n",
        "all_dx_filtered = all_dx_filtered[~all_dx_filtered.str.startswith('Z')]\n",
        "\n",
        "# âœ… Step 4: Count frequencies of remaining ICD-10 codes\n",
        "dx_counts = all_dx_filtered.value_counts().reset_index()\n",
        "dx_counts.columns = ['icd10_code', 'count']\n",
        "dx_counts['percent'] = dx_counts['count'] / dx_counts['count'].sum() * 100\n",
        "\n",
        "# ðŸ“‚ Step 5: Load ICDâ€‘10 code descriptions (update filename if needed)\n",
        "icd_lookup = pd.read_excel('/content/section111validicd10-jan2025_0.xlsx')\n",
        "\n",
        "# ðŸ§¹ Clean and standardize\n",
        "icd_lookup.columns = icd_lookup.columns.str.lower().str.strip()\n",
        "icd_lookup = icd_lookup.rename(columns={\n",
        "    'code': 'icd10_code',\n",
        "    'long description (valid icd-10 fy2025)': 'description'\n",
        "})\n",
        "icd_lookup['icd10_code'] = icd_lookup['icd10_code'].str.upper().str.strip()\n",
        "dx_counts['icd10_code'] = dx_counts['icd10_code'].str.upper().str.strip()\n",
        "\n",
        "# ðŸ”— Step 6: Merge diagnosis counts with descriptions\n",
        "merged = dx_counts.merge(icd_lookup[['icd10_code', 'description']], on='icd10_code', how='left')\n",
        "\n",
        "# âœ… Step 7: Drop NaN descriptions (this removes missing rows)\n",
        "merged = merged.dropna(subset=['description'])\n",
        "\n",
        "# ðŸ“Š Step 8: Display top 30 diagnoses\n",
        "merged_top30 = merged[['icd10_code', 'description', 'count', 'percent']].head(30)\n",
        "merged_top30\n"
      ],
      "metadata": {
        "id": "rctKXdUDRUol"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import textwrap\n",
        "\n",
        "# Drop rows with missing descriptions\n",
        "filtered = merged.dropna(subset=['description'])\n",
        "\n",
        "# Remove Z codes (non-disease administrative/status codes)\n",
        "filtered = filtered[~filtered['icd10_code'].str.startswith('Z', na=False)]\n",
        "\n",
        "# Take top 20\n",
        "top_dx = filtered.head(30).copy()\n",
        "top_dx['description'] = top_dx['description'].astype(str)\n",
        "\n",
        "# Wrap long descriptions for better visibility\n",
        "top_dx['wrapped_desc'] = top_dx['description'].apply(lambda x: '\\n'.join(textwrap.wrap(x, width=50)))\n",
        "\n",
        "# Plot with larger fonts\n",
        "plt.figure(figsize=(14, 20))\n",
        "plt.barh(top_dx['wrapped_desc'], top_dx['count'])\n",
        "plt.xlabel('Frequency', fontsize=14)\n",
        "plt.title('Top 30 Diagnoses Among AD Patients Died In Hospital', fontsize=20)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7AHBILV3Y_5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y shap numpy scikit-learn xgboost\n",
        "!pip install numpy==1.26.4 shap==0.44.1 scikit-learn==1.4.2 xgboost==2.1.1 matplotlib==3.9.2\n"
      ],
      "metadata": {
        "id": "q2qVDQyP7tuC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ================================================================\n",
        "# MACHINE LEARNING PIPELINE (COLABâ€‘FRIENDLY, FULLY FIXED)\n",
        "# ================================================================\n",
        "\n",
        "import os, warnings, numpy as np, pandas as pd\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
        "from xgboost import XGBClassifier\n",
        "import shap\n",
        "\n",
        "# ==== 0) I/O SETUP ======================================================\n",
        "USE_DRIVE = True\n",
        "PATH_DTA = \"/content/NIS_2017_Core data age_over_60 new labels_alz-only for analysis.dta\"\n",
        "\n",
        "if USE_DRIVE:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive', force_remount=True)\n",
        "else:\n",
        "    from google.colab import files\n",
        "    up = files.upload()\n",
        "    PATH_DTA = list(up.keys())[0]\n",
        "\n",
        "# ==== 1) SAFE LOADER =====================================================\n",
        "def read_stata_robust(path):\n",
        "    \"\"\"Read .dta file safely, fallback to latinâ€‘1 if needed.\"\"\"\n",
        "    try:\n",
        "        return pd.read_stata(path, convert_categoricals=False, encoding=\"latin-1\")\n",
        "    except TypeError:\n",
        "        return pd.read_stata(path, convert_categoricals=False)\n",
        "\n",
        "df = read_stata_robust(PATH_DTA)\n",
        "df.columns = df.columns.str.lower()\n",
        "\n",
        "# Deâ€‘duplicate columns (Stata exports often duplicate 'female', 'race')\n",
        "if pd.Index(df.columns).duplicated().any():\n",
        "    dup_names = pd.Series(df.columns)[pd.Index(df.columns).duplicated(keep=False)].unique().tolist()\n",
        "    print(\"âš ï¸ Duplicate column names detected; keeping the first occurrence for:\", dup_names)\n",
        "    df = df.loc[:, ~pd.Index(df.columns).duplicated()].copy()\n",
        "\n",
        "def as_series(x):\n",
        "    import pandas as pd\n",
        "    return x.iloc[:, 0] if isinstance(x, pd.DataFrame) else x\n",
        "\n",
        "# ==== 2) VARIABLE DECLARATIONS ==========================================\n",
        "TARGET = \"died\"\n",
        "GROUPS = \"hosp_nis\"\n",
        "WEIGHT = \"discwt\"\n",
        "\n",
        "expected_binary = [\n",
        "    \"female\",\"elective\",\"aweekend\",\"sepsis\",\"arf\",\"aki\",\"uti\",\"asp\",\"malnut\",\n",
        "    \"dysph\",\"pressulc\",\"chf\",\"cad\",\"afib\",\"cva\",\"anemia\",\"hypoth\",\"dnr\",\"pall\"\n",
        "]\n",
        "expected_cats = [\"race\",\"zipinc_qrtl\",\"tran_in\",\"hosp_division\"]\n",
        "expected_cont = [\"age\"]\n",
        "\n",
        "# ==== 3) BASIC CLEANING =================================================\n",
        "all_feature_names = expected_cont + expected_binary + expected_cats\n",
        "avail = [c for c in all_feature_names if c in df.columns]\n",
        "missing = sorted(set(all_feature_names) - set(avail))\n",
        "if missing:\n",
        "    print(\"âš ï¸ Missing columns (proceeding without them):\", missing)\n",
        "\n",
        "keep = [c for c in [TARGET, GROUPS, WEIGHT] if c in df.columns] + avail\n",
        "df = df[keep].copy()\n",
        "\n",
        "# Safe numeric conversion\n",
        "for c in expected_cont + expected_binary + expected_cats:\n",
        "    if c in df:\n",
        "        s = as_series(df[c])\n",
        "        df[c] = pd.to_numeric(s, errors=\"coerce\")\n",
        "\n",
        "# ==== 4) TARGET FIX =====================================================\n",
        "y_raw = pd.to_numeric(df[TARGET], errors=\"coerce\")\n",
        "y = np.where(y_raw >= 1, 1, 0).astype(int)\n",
        "print(\"âœ… Target cleaned. Unique values after recoding:\", np.unique(y))\n",
        "\n",
        "# ==== 5) GROUPS & WEIGHTS ===============================================\n",
        "groups = pd.to_numeric(df.get(GROUPS, pd.Series(np.arange(len(df)), index=df.index)),\n",
        "                       errors=\"coerce\").fillna(-1).astype(int).values\n",
        "\n",
        "w = pd.to_numeric(df.get(WEIGHT, pd.Series(1, index=df.index)), errors=\"coerce\").fillna(1.0)\n",
        "w = w.replace([np.inf, -np.inf], np.nan).fillna(1e-6)\n",
        "w = w.clip(lower=1e-6, upper=w.quantile(0.999))  # cap extreme outliers\n",
        "w = w.astype(float).values\n",
        "print(f\"âœ… Final cleaned weights: min={w.min():.4e}, max={w.max():.4e}\")\n",
        "\n",
        "# ==== 6) FEATURES =======================================================\n",
        "X = df[avail].copy()\n",
        "for c in X.columns:\n",
        "    if c in expected_cont:\n",
        "        X[c] = X[c].astype(float).fillna(X[c].median())\n",
        "    elif c in expected_binary:\n",
        "        X[c] = X[c].fillna(0).astype(int)\n",
        "    elif c in expected_cats:\n",
        "        X[c] = X[c].fillna(X[c].mode(dropna=True).iloc[0] if X[c].notna().any() else 0).astype(int)\n",
        "    else:\n",
        "        X[c] = X[c].fillna(0)\n",
        "\n",
        "num_feats = [c for c in expected_cont if c in X] + [c for c in expected_binary if c in X]\n",
        "cat_feats = [c for c in expected_cats if c in X]\n",
        "\n",
        "# ==== 7) PREPROCESSOR ===================================================\n",
        "pre = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", StandardScaler(), num_feats),\n",
        "        (\"cat\", \"passthrough\", cat_feats)\n",
        "    ],\n",
        "    remainder=\"drop\"\n",
        ")\n",
        "\n",
        "# ==== 8) MODEL TRAINER (SAFE WEIGHTS) ==================================\n",
        "def fit_with_weights(pipe, X_tr, y_tr, w_tr):\n",
        "    w_tr = pd.Series(w_tr).replace([np.inf, -np.inf], np.nan).fillna(1e-6).clip(lower=1e-6, upper=100)\n",
        "    try:\n",
        "        pipe.fit(X_tr, y_tr, clf__sample_weight=w_tr)\n",
        "    except TypeError:\n",
        "        pipe.fit(X_tr, y_tr)\n",
        "    return pipe\n",
        "\n",
        "# ==== 9) MODELS =========================================================\n",
        "logit = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", LogisticRegression(solver=\"lbfgs\", max_iter=1000))\n",
        "])\n",
        "\n",
        "xgb = Pipeline([\n",
        "    (\"pre\", pre),\n",
        "    (\"clf\", XGBClassifier(\n",
        "        n_estimators=600, learning_rate=0.05, max_depth=4,\n",
        "        subsample=0.9, colsample_bytree=0.9,\n",
        "        reg_lambda=1.0, reg_alpha=0.0,\n",
        "        random_state=42, tree_method=\"hist\", n_jobs=-1,\n",
        "        eval_metric=\"logloss\"\n",
        "    ))\n",
        "])\n",
        "\n",
        "MODELS = {\"Logistic\": logit, \"XGBoost\": xgb}\n",
        "\n",
        "# ==== 10) METRICS =======================================================\n",
        "def weighted_metrics(y_true, y_prob, sample_weight):\n",
        "    y_true = np.asarray(y_true); y_prob = np.asarray(y_prob); sw = np.asarray(sample_weight)\n",
        "    eps = 1e-9\n",
        "    return dict(\n",
        "        AUROC = roc_auc_score(y_true, y_prob, sample_weight=sw),\n",
        "        AUPRC = average_precision_score(y_true, y_prob, sample_weight=sw),\n",
        "        Brier = brier_score_loss(y_true, np.clip(y_prob, eps, 1-eps), sample_weight=sw),\n",
        "        LogLoss = log_loss(y_true, np.clip(y_prob, eps, 1-eps), sample_weight=sw)\n",
        "    )\n",
        "\n",
        "# ==== 11) GROUPED CV ====================================================\n",
        "N_SPLITS = 5\n",
        "gkf = GroupKFold(n_splits=N_SPLITS)\n",
        "\n",
        "summaries, oof_store = [], {}\n",
        "\n",
        "for name, pipe in MODELS.items():\n",
        "    print(f\"\\nâ–¶ Running {name} ...\")\n",
        "    y_all=[]; p_all=[]; w_all=[]; fold_id=[]; idx_all=[]\n",
        "    for fold, (tr, te) in enumerate(gkf.split(X, y, groups=groups), start=1):\n",
        "        X_tr, X_te = X.iloc[tr], X.iloc[te]\n",
        "        y_tr, y_te = y[tr], y[te]\n",
        "        w_tr, w_te = w[tr], w[te]\n",
        "        fitted = fit_with_weights(pipe, X_tr, y_tr, w_tr)\n",
        "        p_te = fitted.predict_proba(X_te)[:,1]\n",
        "        y_all.append(y_te); p_all.append(p_te); w_all.append(w_te)\n",
        "        fold_id.append(np.full_like(y_te, fold)); idx_all.append(te)\n",
        "\n",
        "    y_all = np.concatenate(y_all); p_all = np.concatenate(p_all)\n",
        "    w_all = np.concatenate(w_all); fold_id = np.concatenate(fold_id); idx_all = np.concatenate(idx_all)\n",
        "    met = weighted_metrics(y_all, p_all, w_all)\n",
        "    summaries.append({\"Model\": name, **{k: float(v) for k,v in met.items()}})\n",
        "    oof_store[name] = pd.DataFrame({\"idx\": idx_all, \"fold\": fold_id, \"y\": y_all, \"p\": p_all, \"w\": w_all}).sort_values(\"idx\")\n",
        "\n",
        "summary_df = pd.DataFrame(summaries).sort_values(\"AUROC\", ascending=False)\n",
        "print(\"\\n=== Crossâ€‘validated, weighted metrics (OOF) ===\")\n",
        "print(summary_df.to_string(index=False, formatters={k: \"{:.4f}\".format for k in [\"AUROC\",\"AUPRC\",\"Brier\",\"LogLoss\"]}))\n",
        "\n",
        "for k, df_oof in oof_store.items():\n",
        "    outp = f\"/content/{k}_oof_predictions.csv\"\n",
        "    df_oof.to_csv(outp, index=False)\n",
        "    print(\"Saved:\", outp)\n",
        "\n",
        "# ==== 12) INTERPRETATION ================================================\n",
        "print(\"\\n=== Logistic regression coefficients (full fit) ===\")\n",
        "logit_full = fit_with_weights(MODELS[\"Logistic\"], X, y, w)\n",
        "feat_names = num_feats + cat_feats\n",
        "coef = logit_full.named_steps[\"clf\"].coef_.ravel()\n",
        "logit_imp = pd.DataFrame({\"feature\": feat_names, \"coef\": coef, \"abs_coef\": np.abs(coef)}).sort_values(\"abs_coef\", ascending=False)\n",
        "print(logit_imp.head(25).to_string(index=False))\n",
        "\n",
        "if \"XGBoost\" in MODELS:\n",
        "    print(\"\\n=== XGBoost feature importance (gain) ===\")\n",
        "    xgb_full = fit_with_weights(MODELS[\"XGBoost\"], X, y, w)\n",
        "    xgb_feat_names = feat_names\n",
        "    booster = xgb_full.named_steps[\"clf\"]\n",
        "    gain_scores = booster.get_booster().get_score(importance_type=\"gain\")\n",
        "    rows = [{\"feature\": fname, \"gain\": gain_scores.get(f\"f{i}\", 0.0)} for i, fname in enumerate(xgb_feat_names)]\n",
        "    xgb_imp = pd.DataFrame(rows).sort_values(\"gain\", ascending=False)\n",
        "    print(xgb_imp.head(25).to_string(index=False))\n",
        "\n",
        "    # Get transformed feature matrix and reattach column names\n",
        "X_mat = xgb_full.named_steps[\"pre\"].transform(X)\n",
        "\n",
        "# Combine numerical + categorical feature names after preprocessing\n",
        "transformed_feature_names = (\n",
        "    xgb_full.named_steps[\"pre\"]\n",
        "    .transformers_[0][2] +  # StandardScaler features\n",
        "    xgb_full.named_steps[\"pre\"]\n",
        "    .transformers_[1][2]    # Categorical passthrough\n",
        ")\n",
        "\n",
        "# Turn it into a DataFrame with feature names\n",
        "X_df = pd.DataFrame(X_mat, columns=transformed_feature_names)\n",
        "\n",
        "\n",
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Re-transform your X into DataFrame with numeric values\n",
        "X_mat = xgb_full.named_steps[\"pre\"].transform(X)\n",
        "\n",
        "# Get column names from the preprocessor\n",
        "num_feats = xgb_full.named_steps[\"pre\"].transformers_[0][2]  # numerical\n",
        "cat_feats = xgb_full.named_steps[\"pre\"].transformers_[1][2]  # categorical\n",
        "all_feats = num_feats + cat_feats\n",
        "\n",
        "# Ensure numeric conversion from string, remove brackets if needed\n",
        "def sanitize(x):\n",
        "    if isinstance(x, str):\n",
        "        x = x.replace('[', '').replace(']', '')\n",
        "    try:\n",
        "        return float(x)\n",
        "    except:\n",
        "        return np.nan\n",
        "\n",
        "X_mat_clean = np.vectorize(sanitize)(X_mat)  # Converts all to float\n",
        "X_df = pd.DataFrame(X_mat_clean, columns=all_feats)\n",
        "\n",
        "# Sample subset for SHAP\n",
        "n_sample = min(5000, len(X_df))\n",
        "idx = np.random.default_rng(42).choice(len(X_df), size=n_sample, replace=False)\n",
        "X_sample = X_df.iloc[idx].copy()\n",
        "\n",
        "# Fit SHAP explainer on the XGBoost model\n",
        "explainer = shap.Explainer(xgb_full.named_steps[\"clf\"])\n",
        "shap_values = explainer(X_sample)\n",
        "\n",
        "# Plot SHAP summary\n",
        "# === Custom Labeled SHAP Bar Plot ===\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute mean absolute SHAP values for each feature\n",
        "mean_shap_vals = np.abs(shap_values.values).mean(axis=0)\n",
        "shap_df = pd.DataFrame({\n",
        "    \"feature\": X_sample.columns,\n",
        "    \"mean_abs_shap\": mean_shap_vals\n",
        "}).sort_values(\"mean_abs_shap\", ascending=False)\n",
        "\n",
        "# Define your custom readable labels for the top features\n",
        "\n",
        "label_dict = {\n",
        "    \"female\": \"Sex\",\n",
        "    \"elective\": \"Elective Admission\",\n",
        "    \"aweekend\": \"Weekend Admission\",\n",
        "    \"sepsis\": \"Sepsis\",\n",
        "    \"arf\": \"Acute Respiratory Failure\",\n",
        "    \"aki\": \"Acute Kidney Injury\",\n",
        "    'age': 'Age',\n",
        "    \"uti\": \"Urinary Tract Infection\",\n",
        "    \"asp\": \"Aspiration Pneumonia\",\n",
        "    \"malnut\": \"Malnutrition\",\n",
        "    \"dysph\": \"Dysphagia\",\n",
        "    \"pressulc\": \"Pressure Ulcer\",\n",
        "    \"chf\": \"Congestive Heart Failure\",\n",
        "    \"cad\": \"Coronary Artery Disease\",\n",
        "    \"afib\": \"Atrial Fibrillation\",\n",
        "    \"cva\": \"Stroke (Cerebrovascular Accident)\",\n",
        "    \"anemia\": \"Anemia\",\n",
        "    \"hypoth\": \"Hypothyroidism\",\n",
        "    \"dnr\": \"Do Not Resuscitate\",\n",
        "    \"pall\": \"Palliative Care\",\n",
        "    \"race\": \"Race/Ethnicity\",\n",
        "    \"zipinc_qrtl\": \"Income Quartile (by ZIP Code)\",\n",
        "    \"tran_in\": \"Transferred In\",\n",
        "    \"hosp_division\": \"Hospital Region\"\n",
        "}\n",
        "\n",
        "\n",
        "# Map labels, keeping original names if no match\n",
        "shap_df['label'] = shap_df['feature'].map(label_dict).fillna(shap_df['feature'])\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.barh(shap_df['label'].iloc[::-1], shap_df['mean_abs_shap'].iloc[::-1], color='dodgerblue')\n",
        "plt.xlabel(\"mean(|SHAP value|) (average impact on model output magnitude)\")\n",
        "plt.title(\"SHAP Feature Importance (Top Predictors of Mortality)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# ==== 13) THRESHOLD METRICS =============================================\n",
        "def best_threshold_by_youden(y_true, y_prob, sample_weight=None):\n",
        "    from sklearn.metrics import roc_curve\n",
        "    fpr, tpr, thr = roc_curve(y_true, y_prob, sample_weight=sample_weight)\n",
        "    j = tpr - fpr\n",
        "    i = np.argmax(j)\n",
        "    return float(thr[i]), float(tpr[i]), float(fpr[i])\n",
        "\n",
        "for name, df_oof in oof_store.items():\n",
        "    thr, tpr, fpr = best_threshold_by_youden(df_oof[\"y\"], df_oof[\"p\"], df_oof[\"w\"])\n",
        "    print(f\"{name}: Youdenâ€‘optimal threshold ~ {thr:.3f} (TPR={tpr:.3f}, FPR={fpr:.3f})\")\n",
        "\n",
        "summary_df.to_csv(\"/content/ML_cv_summary.csv\", index=False)\n",
        "print(\"\\nâœ… Saved summary: /content/ML_cv_summary.csv\")\n"
      ],
      "metadata": {
        "id": "lYeFn1rX5hEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Define full feature name mapping\n",
        "feature_name_map = {\n",
        "    'pall': 'Palliative Care',\n",
        "    'zipinc_qrtl': 'ZIP Income Quartile',\n",
        "    'arf': 'Acute Respiratory Failure',\n",
        "    'aki': 'Acute Kidney Injury',\n",
        "    'age': 'Age',\n",
        "    'female': 'Sex',\n",
        "    'race': 'Race',\n",
        "    'sepsis': 'Sepsis',\n",
        "    'chf': 'Congestive Heart Failure',\n",
        "    'asp': 'Aspiration Pneumonia',\n",
        "    'aweekend': 'Weekend Admission',\n",
        "    'malnut': 'Malnutrition',\n",
        "    'uti': 'Urinary Tract Infection',\n",
        "    'tran_in': 'Transferred In',\n",
        "    'dysph': 'Dysphagia',\n",
        "    'cad': 'Coronary Artery Disease',\n",
        "    'dnr': 'DNR Status',\n",
        "    'hypoth': 'Hypothyroidism',\n",
        "    'pressulc': 'Pressure Ulcer',\n",
        "    'afib': 'Atrial Fibrillation'\n",
        "}\n",
        "\n",
        "# Step 2: Transform input and sample\n",
        "X_mat = xgb_full.named_steps['pre'].transform(X)\n",
        "X_mat = pd.DataFrame(X_mat, columns=xgb_feat_names)\n",
        "X_sample = X_mat.sample(n=1000, random_state=42)\n",
        "\n",
        "# Step 3: Compute SHAP values\n",
        "explainer = shap.Explainer(xgb_full.named_steps['clf'])\n",
        "shap_values = explainer(X_sample)\n",
        "\n",
        "# Step 4: Filter top features\n",
        "top_features = list(feature_name_map.keys())\n",
        "top_indices = [i for i, f in enumerate(shap_values.feature_names) if f in top_features]\n",
        "shap_values_filtered = shap_values[:, top_indices]\n",
        "\n",
        "# Step 5: Rename features inside SHAP object\n",
        "# This replaces the feature names in-place (works on the view, not original)\n",
        "for i, idx in enumerate(top_indices):\n",
        "    shap_values_filtered.feature_names[i] = feature_name_map[shap_values.feature_names[idx]]\n",
        "\n",
        "# Step 6: Plot beeswarm with full names\n",
        "shap.plots.beeswarm(shap_values_filtered, max_display=20)\n"
      ],
      "metadata": {
        "id": "h6uB7WgbskXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SHAP Mean Absolute Values**"
      ],
      "metadata": {
        "id": "Lxa_SsXFn4Cj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "shap_df = pd.DataFrame({\n",
        "    \"Feature\": shap_values.feature_names,\n",
        "    \"SHAP_Mean_Abs\": np.abs(shap_values.values).mean(axis=0)\n",
        "}).sort_values(\"SHAP_Mean_Abs\", ascending=False)\n",
        "display(shap_df)"
      ],
      "metadata": {
        "id": "wAQAKqGqmhSM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
        "\n",
        "# Drop dnr and pall columns\n",
        "X_sensitivity = X.drop(columns=[\"dnr\", \"pall\"])  # X is your original full feature matrix\n",
        "y = y  # already binary 0/1 mortality\n",
        "\n",
        "# Fit logistic regression\n",
        "lr_sens = LogisticRegression(max_iter=1000)\n",
        "lr_sens.fit(X_sensitivity, y)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_lr_sens = lr_sens.predict_proba(X_sensitivity)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "auroc_lr_sens = roc_auc_score(y, y_pred_lr_sens)\n",
        "auprc_lr_sens = average_precision_score(y, y_pred_lr_sens)\n",
        "brier_lr_sens = brier_score_loss(y, y_pred_lr_sens)\n",
        "logloss_lr_sens = log_loss(y, y_pred_lr_sens)\n",
        "\n",
        "# Print results\n",
        "print(f\"Sensitivity Logistic Regression AUROC:   {auroc_lr_sens:.4f}\")\n",
        "print(f\"Sensitivity Logistic Regression AUPRC:   {auprc_lr_sens:.4f}\")\n",
        "print(f\"Sensitivity Logistic Regression Brier:   {brier_lr_sens:.4f}\")\n",
        "print(f\"Sensitivity Logistic Regression LogLoss: {logloss_lr_sens:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Frj8MLSzS-DK",
        "outputId": "d6b87d70-6bed-4f54-d75d-87ca2e9b4ba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity Logistic Regression AUROC:   0.8075\n",
            "Sensitivity Logistic Regression AUPRC:   0.2078\n",
            "Sensitivity Logistic Regression Brier:   0.0402\n",
            "Sensitivity Logistic Regression LogLoss: 0.1566\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "\n",
        "# Fit XGBoost classifier\n",
        "xgb_sens = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "xgb_sens.fit(X_sensitivity, y)\n",
        "\n",
        "# Predict probabilities\n",
        "y_pred_xgb_sens = xgb_sens.predict_proba(X_sensitivity)[:, 1]\n",
        "\n",
        "# Compute metrics\n",
        "auroc_xgb_sens = roc_auc_score(y, y_pred_xgb_sens)\n",
        "auprc_xgb_sens = average_precision_score(y, y_pred_xgb_sens)\n",
        "brier_xgb_sens = brier_score_loss(y, y_pred_xgb_sens)\n",
        "logloss_xgb_sens = log_loss(y, y_pred_xgb_sens)\n",
        "\n",
        "# Print results\n",
        "print(f\"Sensitivity XGBoost AUROC:   {auroc_xgb_sens:.4f}\")\n",
        "print(f\"Sensitivity XGBoost AUPRC:   {auprc_xgb_sens:.4f}\")\n",
        "print(f\"Sensitivity XGBoost Brier:   {brier_xgb_sens:.4f}\")\n",
        "print(f\"Sensitivity XGBoost LogLoss: {logloss_xgb_sens:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6H5D9G6vS9_D",
        "outputId": "82d9c0ce-4306-4d2d-8fc5-055fd5d5e065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sensitivity XGBoost AUROC:   0.8950\n",
            "Sensitivity XGBoost AUPRC:   0.5055\n",
            "Sensitivity XGBoost Brier:   0.0323\n",
            "Sensitivity XGBoost LogLoss: 0.1241\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shap\n",
        "\n",
        "explainer_sens = shap.Explainer(xgb_sens, X_sensitivity)\n",
        "shap_values_sens = explainer_sens(X_sensitivity)\n",
        "\n",
        "# Plot\n",
        "shap.plots.bar(shap_values_sens, max_display=25)\n"
      ],
      "metadata": {
        "id": "KFXJAdxFaG8N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "label_map = {\n",
        "    'arf': 'Acute Respiratory Failure',\n",
        "    'age': 'Age',\n",
        "    'sepsis': 'Sepsis',\n",
        "    'uti': 'Urinary Tract Infection',\n",
        "    'aki': 'Acute Kidney Injury',\n",
        "    'hosp_division': 'Hospital Division',\n",
        "    'tran_in': 'Transferred from Another Facility',\n",
        "    'asp': 'Aspiration Pneumonia',\n",
        "    'elective': 'Elective Admission',\n",
        "    'race': 'Race/Ethnicity',\n",
        "    'zipinc_qrtl': 'Income Quartile (ZIP)',\n",
        "    'chf': 'Congestive Heart Failure',\n",
        "    'afib': 'Atrial Fibrillation',\n",
        "    'anemia': 'Anemia',\n",
        "    'malnut': 'Malnutrition',\n",
        "    'dysph': 'Dysphagia',\n",
        "    'pressulc': 'Pressure Ulcer',\n",
        "    'cad': 'Coronary Artery Disease',\n",
        "    'cva': 'Cerebrovascular Disease',\n",
        "    'hypoth': 'Hypothyroidism',\n",
        "    'dnr': 'Do-Not-Resuscitate',\n",
        "    'pall': 'Palliative Care',\n",
        "    'aweekend': 'Weekend Admission',\n",
        "    'female': 'Female'\n",
        "}\n",
        "\n",
        "shap_df['Feature_Full'] = shap_df['Feature'].map(label_map)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "yBIAHZnZoq02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Sort by SHAP value (descending)\n",
        "shap_df = shap_df.sort_values(by='SHAP_Mean_Abs', ascending=True)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(12, 9))\n",
        "plt.barh(shap_df['Feature_Full'], shap_df['SHAP_Mean_Abs'], color='deeppink')\n",
        "\n",
        "# Larger font size for labels\n",
        "plt.xlabel('mean(|SHAP value|)', fontsize=16)\n",
        "plt.ylabel('Predictors', fontsize=16)\n",
        "plt.title(\"Top Predictors of In-Hospital Mortality in AD (Sensitivity Analysis)\", fontsize=18)\n",
        "\n",
        "# Optionally increase tick label size\n",
        "plt.xticks(fontsize=14)\n",
        "plt.yticks(fontsize=14)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "b__krOZGxgZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Column lists (use yours)\n",
        "num_cols = [\"age\"]  # continuous\n",
        "bin_cols = [  # binary flags\n",
        "    \"female\",\"elective\",\"aweekend\",\"sepsis\",\"arf\",\"aki\",\"uti\",\"asp\",\"malnut\",\n",
        "    \"dysph\",\"pressulc\",\"chf\",\"cad\",\"afib\",\"cva\",\"anemia\",\"hypoth\",\"dnr\",\"pall\"\n",
        "]\n",
        "cat_cols = [\"race\",\"zipinc_qrtl\",\"tran_in\",\"hosp_division\"]  # coded categories\n",
        "\n",
        "# Keep only those that exist\n",
        "num_cols = [c for c in num_cols if c in X.columns]\n",
        "bin_cols = [c for c in bin_cols if c in X.columns]\n",
        "cat_cols = [c for c in cat_cols if c in X.columns]\n",
        "\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler(with_mean=False)),\n",
        "])\n",
        "\n",
        "binary_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
        "    (\"scaler\", StandardScaler(with_mean=False)),  # optional; helps LR; harmless for XGB\n",
        "])\n",
        "\n",
        "cat_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "])\n",
        "\n",
        "preprocess = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_pipe, num_cols),\n",
        "        (\"bin\", binary_pipe, bin_cols),\n",
        "        (\"cat\", cat_pipe, cat_cols),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    sparse_threshold=0.3,\n",
        ")\n"
      ],
      "metadata": {
        "id": "MLa42gFApNj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GroupKFold\n",
        "from sklearn.base import clone\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss, log_loss\n",
        "\n",
        "def groupkfold_oof(pipe, X, y, groups, w=None, n_splits=5):\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "    oof = np.full(len(y), np.nan, dtype=float)\n",
        "\n",
        "    for fold, (tr, te) in enumerate(gkf.split(X, y, groups), start=1):\n",
        "        model = clone(pipe)\n",
        "        fit_params = {}\n",
        "        if w is not None:\n",
        "            fit_params[\"clf__sample_weight\"] = w[tr]\n",
        "        model.fit(X.iloc[tr], y[tr], **fit_params)\n",
        "        oof[te] = model.predict_proba(X.iloc[te])[:, 1]\n",
        "        print(f\"Fold {fold}/{n_splits}: te={len(te)} deaths={int(y[te].sum())}\")\n",
        "\n",
        "    return oof\n",
        "\n",
        "def summarize_metrics(y_true, y_prob, w=None):\n",
        "    return {\n",
        "        \"AUROC\": roc_auc_score(y_true, y_prob, sample_weight=w),\n",
        "        \"AUPRC\": average_precision_score(y_true, y_prob, sample_weight=w),\n",
        "        \"Brier\": brier_score_loss(y_true, y_prob, sample_weight=w),\n",
        "        \"LogLoss\": log_loss(y_true, y_prob, sample_weight=w),\n",
        "    }\n"
      ],
      "metadata": {
        "id": "fLeVMv_ypWJv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "def make_preprocess(X, expected_cont, expected_binary, expected_cats):\n",
        "    num_cols = [c for c in expected_cont   if c in X.columns]\n",
        "    bin_cols = [c for c in expected_binary if c in X.columns]\n",
        "    cat_cols = [c for c in expected_cats   if c in X.columns]\n",
        "\n",
        "    numeric_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler(with_mean=False)),\n",
        "    ])\n",
        "\n",
        "    binary_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=0)),\n",
        "        (\"scaler\", StandardScaler(with_mean=False)),\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline([\n",
        "        (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    ])\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        transformers=[\n",
        "            (\"num\", numeric_pipe, num_cols),\n",
        "            (\"bin\", binary_pipe, bin_cols),\n",
        "            (\"cat\", cat_pipe, cat_cols),\n",
        "        ],\n",
        "        remainder=\"drop\",\n",
        "        sparse_threshold=0.3\n",
        "    )\n",
        "    return pre\n"
      ],
      "metadata": {
        "id": "yjZPZi04sBtO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# ---- FULL ----\n",
        "pre_full = make_preprocess(X, expected_cont, expected_binary, expected_cats)\n",
        "\n",
        "lr_pipe_full = Pipeline([\n",
        "    (\"preprocess\", pre_full),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "xgb_pipe_full = Pipeline([\n",
        "    (\"preprocess\", pre_full),\n",
        "    (\"clf\", XGBClassifier(\n",
        "        n_estimators=600, learning_rate=0.05, max_depth=4,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
        "        n_jobs=-1, random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "p_lr_full  = groupkfold_oof(lr_pipe_full,  X, y, groups, w=w, n_splits=5)\n",
        "p_xgb_full = groupkfold_oof(xgb_pipe_full, X, y, groups, w=w, n_splits=5)\n",
        "\n",
        "print(\"FULL LR :\", summarize_metrics(y, p_lr_full,  w=w))\n",
        "print(\"FULL XGB:\", summarize_metrics(y, p_xgb_full, w=w))\n",
        "\n",
        "\n",
        "# ---- RESTRICTED (drop DNR + pall) ----\n",
        "X_restrict = X.drop(columns=[\"dnr\", \"pall\"], errors=\"ignore\")\n",
        "\n",
        "pre_res = make_preprocess(X_restrict, expected_cont, expected_binary, expected_cats)\n",
        "\n",
        "lr_pipe_res = Pipeline([\n",
        "    (\"preprocess\", pre_res),\n",
        "    (\"clf\", LogisticRegression(max_iter=2000, solver=\"lbfgs\"))\n",
        "])\n",
        "\n",
        "xgb_pipe_res = Pipeline([\n",
        "    (\"preprocess\", pre_res),\n",
        "    (\"clf\", XGBClassifier(\n",
        "        n_estimators=600, learning_rate=0.05, max_depth=4,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
        "        n_jobs=-1, random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "p_lr_res  = groupkfold_oof(lr_pipe_res,  X_restrict, y, groups, w=w, n_splits=5)\n",
        "p_xgb_res = groupkfold_oof(xgb_pipe_res, X_restrict, y, groups, w=w, n_splits=5)\n",
        "\n",
        "print(\"RESTRICTED LR :\", summarize_metrics(y, p_lr_res,  w=w))\n",
        "print(\"RESTRICTED XGB:\", summarize_metrics(y, p_xgb_res, w=w))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJH8bFPssH_g",
        "outputId": "6f67c1d8-0468-4d4d-ed78-69ae6ad7fd4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fold 1/5: te=17867 deaths=819\n",
            "Fold 2/5: te=17866 deaths=872\n",
            "Fold 3/5: te=17866 deaths=784\n",
            "Fold 4/5: te=17866 deaths=844\n",
            "Fold 5/5: te=17866 deaths=839\n",
            "Fold 1/5: te=17867 deaths=819\n",
            "Fold 2/5: te=17866 deaths=872\n",
            "Fold 3/5: te=17866 deaths=784\n",
            "Fold 4/5: te=17866 deaths=844\n",
            "Fold 5/5: te=17866 deaths=839\n",
            "FULL LR : {'AUROC': 0.8789167395195499, 'AUPRC': 0.31015786435016945, 'Brier': 0.03716758786632564, 'LogLoss': 0.13749659168231904}\n",
            "FULL XGB: {'AUROC': 0.8865762799202569, 'AUPRC': 0.3237818133905529, 'Brier': 0.03641064456551178, 'LogLoss': 0.13369076277727807}\n",
            "Fold 1/5: te=17867 deaths=819\n",
            "Fold 2/5: te=17866 deaths=872\n",
            "Fold 3/5: te=17866 deaths=784\n",
            "Fold 4/5: te=17866 deaths=844\n",
            "Fold 5/5: te=17866 deaths=839\n",
            "Fold 1/5: te=17867 deaths=819\n",
            "Fold 2/5: te=17866 deaths=872\n",
            "Fold 3/5: te=17866 deaths=784\n",
            "Fold 4/5: te=17866 deaths=844\n",
            "Fold 5/5: te=17866 deaths=839\n",
            "RESTRICTED LR : {'AUROC': 0.8059365320019443, 'AUPRC': 0.20559556863458672, 'Brier': 0.040274838460998406, 'LogLoss': 0.15688373304843653}\n",
            "RESTRICTED XGB: {'AUROC': 0.8105570410183913, 'AUPRC': 0.2061201969852941, 'Brier': 0.04026639856579602, 'LogLoss': 0.1562744561827663}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dQ_UblvtTtCr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Fit FINAL restricted model\n",
        "# -----------------------------\n",
        "X_restrict = X.drop(columns=[\"dnr\", \"pall\"], errors=\"ignore\")\n",
        "\n",
        "pre_res = make_preprocess(X_restrict, expected_cont, expected_binary, expected_cats)\n",
        "\n",
        "xgb_pipe_res_final = Pipeline([\n",
        "    (\"preprocess\", pre_res),\n",
        "    (\"clf\", XGBClassifier(\n",
        "        n_estimators=600, learning_rate=0.05, max_depth=4,\n",
        "        subsample=0.9, colsample_bytree=0.9, reg_lambda=1.0,\n",
        "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
        "        n_jobs=-1, random_state=42\n",
        "    ))\n",
        "])\n",
        "\n",
        "# If you used discharge weights in training, keep them here too\n",
        "fit_params = {}\n",
        "if w is not None:\n",
        "    fit_params[\"clf__sample_weight\"] = w\n",
        "\n",
        "xgb_pipe_res_final.fit(X_restrict, y, **fit_params)\n",
        "\n",
        "# -----------------------------------\n",
        "# 2) Prepare data for SHAP computation\n",
        "# -----------------------------------\n",
        "pre_fitted = xgb_pipe_res_final.named_steps[\"preprocess\"]\n",
        "model_fitted = xgb_pipe_res_final.named_steps[\"clf\"]\n",
        "\n",
        "# Transform X to the numeric matrix the model sees\n",
        "X_res_trans = pre_fitted.transform(X_restrict)\n",
        "\n",
        "# Feature names after preprocessing (works in sklearn >=1.0)\n",
        "try:\n",
        "    feat_names = pre_fitted.get_feature_names_out()\n",
        "except Exception:\n",
        "    feat_names = [f\"f{i}\" for i in range(X_res_trans.shape[1])]\n",
        "\n",
        "# -----------------------------------\n",
        "# 3) SHAP on a manageable subsample\n",
        "# -----------------------------------\n",
        "np.random.seed(42)\n",
        "n_shap = min(5000, X_res_trans.shape[0])\n",
        "idx = np.random.choice(X_res_trans.shape[0], size=n_shap, replace=False)\n",
        "\n",
        "X_shap = X_res_trans[idx]\n",
        "\n",
        "explainer = shap.TreeExplainer(model_fitted)\n",
        "shap_values = explainer.shap_values(X_shap)\n",
        "\n",
        "# -----------------------------------\n",
        "# 4) Restricted-model SHAP BAR plot\n",
        "# -----------------------------------\n",
        "plt.figure()\n",
        "shap.summary_plot(\n",
        "    shap_values, X_shap,\n",
        "    feature_names=feat_names,\n",
        "    plot_type=\"bar\",\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_restricted_bar_top20.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# -----------------------------------\n",
        "# 5) Restricted-model SHAP BEESWARM\n",
        "# -----------------------------------\n",
        "plt.figure()\n",
        "shap.summary_plot(\n",
        "    shap_values, X_shap,\n",
        "    feature_names=feat_names,\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_restricted_beeswarm_top20.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved:\",\n",
        "      \"SHAP_restricted_bar_top20.png\",\n",
        "      \"SHAP_restricted_beeswarm_top20.png\")\n"
      ],
      "metadata": {
        "id": "nA3xsOk9t5DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kWBSM9_4T3DM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "import shap\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "label_map = {\n",
        "    'arf': 'Acute Respiratory Failure',\n",
        "    'age': 'Age',\n",
        "    'sepsis': 'Sepsis',\n",
        "    'uti': 'Urinary Tract Infection',\n",
        "    'aki': 'Acute Kidney Injury',\n",
        "    'hosp_division': 'Hospital Division',\n",
        "    'tran_in': 'Transferred from Another Facility',\n",
        "    'asp': 'Aspiration Pneumonia',\n",
        "    'elective': 'Elective Admission',\n",
        "    'race': 'Race/Ethnicity',\n",
        "    'zipinc_qrtl': 'Income Quartile (ZIP)',\n",
        "    'chf': 'Congestive Heart Failure',\n",
        "    'afib': 'Atrial Fibrillation',\n",
        "    'anemia': 'Anemia',\n",
        "    'malnut': 'Malnutrition',\n",
        "    'dysph': 'Dysphagia',\n",
        "    'pressulc': 'Pressure Ulcer',\n",
        "    'cad': 'Coronary Artery Disease',\n",
        "    'cva': 'Cerebrovascular Disease',\n",
        "    'hypoth': 'Hypothyroidism',\n",
        "    'aweekend': 'Weekend Admission',\n",
        "    'female': 'Female'\n",
        "}\n",
        "\n",
        "def pretty_name(feat: str) -> str:\n",
        "    \"\"\"\n",
        "    Convert preprocessor feature names like:\n",
        "      bin__arf, num__age, cat__race_2, cat__hosp_division_4\n",
        "    into human readable labels using label_map.\n",
        "    \"\"\"\n",
        "    # Remove sklearn ColumnTransformer prefixes like bin__/num__/cat__\n",
        "    base = re.sub(r'^(bin|num|cat)__', '', feat)\n",
        "\n",
        "    # If OneHotEncoder created dummy columns, they often look like \"race_3\" or \"hosp_division_2\"\n",
        "    # We keep the parent variable label and add category value if present.\n",
        "    m = re.match(r'^(.*)_(\\d+)$', base)\n",
        "    if m:\n",
        "        var, level = m.group(1), m.group(2)\n",
        "        var_label = label_map.get(var, var)\n",
        "        return f\"{var_label}: {level}\"   # simple/neutral; replace with real category names if you have them\n",
        "    else:\n",
        "        return label_map.get(base, base)\n",
        "\n",
        "# --- After you fit the final restricted model and get X_shap, shap_values, feat_names ---\n",
        "# Example:\n",
        "# feat_names = pre_fitted.get_feature_names_out()\n",
        "\n",
        "pretty_feat_names = [pretty_name(f) for f in feat_names]\n",
        "\n",
        "# BAR (Top 20)\n",
        "plt.figure()\n",
        "shap.summary_plot(\n",
        "    shap_values, X_shap,\n",
        "    feature_names=pretty_feat_names,\n",
        "    plot_type=\"bar\",\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_restricted_bar_top20_labeled.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "# BEESWARM (Top 20)\n",
        "plt.figure()\n",
        "shap.summary_plot(\n",
        "    shap_values, X_shap,\n",
        "    feature_names=pretty_feat_names,\n",
        "    max_display=20,\n",
        "    show=False\n",
        ")\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"SHAP_restricted_beeswarm_top20_labeled.png\", dpi=300, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "\n",
        "print(\"Saved labeled plots:\",\n",
        "      \"SHAP_restricted_bar_top20_labeled.png\",\n",
        "      \"SHAP_restricted_beeswarm_top20_labeled.png\")\n"
      ],
      "metadata": {
        "id": "bZ0_eN8J27kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}